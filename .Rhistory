labels = lbls_percent,
col = rainbow(length(lbls_percent)),
main = "Pie Chart by Percentage")
# MATH 3070 – A1
# Name: Hui Wang
# UID: u1303081
# Date: 2025-05-14
# Define student counts by major
ct <- c(12, 5, 48, 25, 10)
lbls <- c("Arts", "Business", "Engineering", "Math", "Physics")
# Create labels showing counts
lbls_count <- paste(lbls, ct, sep = " ")
# Create labels showing percentages
pct <- round(ct / sum(ct) * 100)
lbls_percent <- paste(lbls, pct, "%", sep = " ")
# Pie Chart 1 – by number of students
pie(ct,
labels = lbls_count,
col = c("red", "blue", "green", "yellow", "purple"),
main = "Pie Chart by Number of Students")
# Pie Chart 2 – by percentage
pie(ct,
labels = lbls_percent,
col = rainbow(length(lbls_percent)),
main = "Pie Chart by Percentage")
# Visualize clusters
fviz_cluster(kmeans_result, data = scaled_df,
geom = "point",
ellipse.type = "norm",
ggtheme = theme_minimal(),
main = "K-Means Clustering of E-Moped Users")
install.packages("ggplot2")
install.packages("dplyr")
install.packages("cluster")
install.packages("factoextra")
# Load libraries
library(ggplot2)
library(dplyr)
library(cluster)
library(factoextra)
# Simulated user behavior data
set.seed(42)
df <- data.frame(
avg_duration = c(rnorm(50, mean = 12, sd = 3), rnorm(50, mean = 30, sd = 4), rnorm(50, mean = 20, sd = 2)),
rides_per_week = c(rnorm(50, mean = 10, sd = 2), rnorm(50, mean = 3, sd = 1), rnorm(50, mean = 6, sd = 2)),
weekend_ratio = c(rnorm(50, mean = 0.2, sd = 0.05), rnorm(50, mean = 0.8, sd = 0.1), rnorm(50, mean = 0.5, sd = 0.1))
)
# Scale the data
scaled_df <- scale(df)
# Run k-means clustering
set.seed(123)
kmeans_result <- kmeans(scaled_df, centers = 3, nstart = 25)
# Add cluster labels
df$cluster <- as.factor(kmeans_result$cluster)
# Visualize clusters
fviz_cluster(kmeans_result, data = scaled_df,
geom = "point",
ellipse.type = "norm",
ggtheme = theme_minimal(),
main = "K-Means Clustering of E-Moped Users")
# Cluster summary
cluster_summary <- df %>%
group_by(cluster) %>%
summarise(
avg_duration = mean(avg_duration),
rides_per_week = mean(rides_per_week),
weekend_ratio = mean(weekend_ratio)
)
print(cluster_summary)
# Plot heatmap
ggplot(ride_matrix, aes(x = hour, y = weekday, fill = ride_count)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "#f0f9e8", high = "#0868ac") +
labs(
title = "Ride Frequency Heatmap by Hour and Weekday",
x = "Hour of Day",
y = "Day of Week",
fill = "Ride Count"
) +
theme_minimal()
Install packages if needed:
Install packages if needed:
#
# ============================================
# Ride Time Heatmap – Shared E-Moped Project
# Author: Hui Wang
# Visualizes ride frequency by weekday and hour
# ============================================
# Install packages if needed:
install.packages("ggplot2")
install.packages("dplyr")
install.packages("lubridate")
library(ggplot2)
library(dplyr)
library(lubridate)
# Simulate sample ride start times (300 rows)
set.seed(123)
df <- data.frame(
ride_id = 1:300,
start_time = as.POSIXct("2024-06-01 00:00:00") + runif(300, min = 0, max = 7*24*60*60)
)
df <- df %>%
mutate(
weekday = wday(start_time, label = TRUE, abbr = FALSE),  # Monday to Sunday
hour = hour(start_time)
)
# Aggregate counts
ride_matrix <- df %>%
group_by(weekday, hour) %>%
summarise(ride_count = n(), .groups = "drop")
# Plot heatmap
ggplot(ride_matrix, aes(x = hour, y = weekday, fill = ride_count)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "#f0f9e8", high = "#0868ac") +
labs(
title = "Ride Frequency Heatmap by Hour and Weekday",
x = "Hour of Day",
y = "Day of Week",
fill = "Ride Count"
) +
theme_minimal()
install.packages("dplyr")
install.packages("ggplot2")
# ============================================
# Ride Time Heatmap – Shared E-Moped Project
# Author: Hui Wang
# Visualizes ride frequency by weekday and hour
# ============================================
# Install packages if needed:
# install.packages("ggplot2")
# install.packages("dplyr")
# install.packages("lubridate")
library(ggplot2)
library(dplyr)
library(lubridate)
# Simulate sample ride start times (300 rows)
set.seed(123)
df <- data.frame(
ride_id = 1:300,
start_time = as.POSIXct("2024-06-01 00:00:00") + runif(300, min = 0, max = 7*24*60*60)
)
df <- df %>%
mutate(
weekday = wday(start_time, label = TRUE, abbr = FALSE),  # Monday to Sunday
hour = hour(start_time)
)
# Aggregate counts
ride_matrix <- df %>%
group_by(weekday, hour) %>%
summarise(ride_count = n(), .groups = "drop")
# Plot heatmap
ggplot(ride_matrix, aes(x = hour, y = weekday, fill = ride_count)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "#f0f9e8", high = "#0868ac") +
labs(
title = "Ride Frequency Heatmap by Hour and Weekday",
x = "Hour of Day",
y = "Day of Week",
fill = "Ride Count"
) +
theme_minimal()
setwd("/Users/hui/github/us-job-market-analysis")
library(tidyverse)
library(readr)
# Load cleaned job data
job_data <- read_csv("data/cleaned_job_data.csv")
# Classify each job as Remote / Hybrid / On-site / Unspecified
remote_classified <- job_data %>%
mutate(work_type = case_when(
str_detect(tolower(location), "remote") ~ "Remote",
str_detect(tolower(location), "hybrid|partial|flexible") ~ "Hybrid",
str_detect(tolower(location), "on[- ]?site|office") ~ "On-site",
TRUE ~ "Unspecified"
)) %>%
count(work_type)
# Print result
print(remote_classified)
# Create output folder if it doesn't exist
if (!dir.exists("visualizations")) dir.create("visualizations")
# Plot pie chart
plot <- ggplot(remote_classified, aes(x = "", y = n, fill = work_type)) +
geom_col(width = 1) +
coord_polar("y") +
theme_void() +
labs(title = "Job Location Type Distribution") +
scale_fill_manual(values = c(
"Remote" = "#1f77b4",
"Hybrid" = "#2ca02c",
"On-site" = "#ff7f0e",
"Unspecified" = "#999999"
))
# Save chart
ggsave("visualizations/remote_vs_onsite.png", plot, width = 6, height = 6)
cat("Updated remote/on-site chart saved to visualizations/remote_vs_onsite.png\n")
setwd("/Users/hui/github/us-job-market-analysis")
library(tidyverse)
library(readr)
# Load cleaned job data
job_data <- read_csv("data/cleaned_job_data.csv")
# Classify each job as Remote / Hybrid / On-site / Unspecified
remote_classified <- job_data %>%
mutate(work_type = case_when(
str_detect(tolower(location), "remote") ~ "Remote",
str_detect(tolower(location), "hybrid|partial|flexible") ~ "Hybrid",
str_detect(tolower(location), "on[- ]?site|office") ~ "On-site",
TRUE ~ "Unspecified"
)) %>%
count(work_type)
# Print result
print(remote_classified)
# Create output folder if it doesn't exist
if (!dir.exists("visualizations")) dir.create("visualizations")
# Plot pie chart
plot <- ggplot(remote_classified, aes(x = "", y = n, fill = work_type)) +
geom_col(width = 1) +
coord_polar("y") +
theme_void() +
labs(title = "Job Location Type Distribution") +
scale_fill_manual(values = c(
"Remote" = "#1f77b4",
"Hybrid" = "#2ca02c",
"On-site" = "#ff7f0e",
"Unspecified" = "#999999"
))
# Save chart
ggsave("visualizations/remote_vs_onsite.png", plot, width = 6, height = 6)
cat("Updated remote/on-site chart saved to visualizations/remote_vs_onsite.png\n")
setwd("/Users/hui/github/us-job-market-analysis")
library(tidyverse)
library(readr)
# Load cleaned job data
job_data <- read_csv("data/cleaned_job_data.csv")
# Classify each job as Remote / Hybrid / On-site / Unspecified
remote_classified <- job_data %>%
mutate(work_type = case_when(
str_detect(tolower(location), "remote") ~ "Remote",
str_detect(tolower(location), "hybrid|partial|flexible") ~ "Hybrid",
str_detect(tolower(location), "on[- ]?site|office") ~ "On-site",
TRUE ~ "Unspecified"
)) %>%
count(work_type)
# Print result
print(remote_classified)
# Create output folder if it doesn't exist
if (!dir.exists("visualizations")) dir.create("visualizations")
# Plot pie chart
plot <- ggplot(remote_classified, aes(x = "", y = n, fill = work_type)) +
geom_col(width = 1) +
coord_polar("y") +
theme_void() +
labs(title = "Job Location Type Distribution") +
scale_fill_manual(values = c(
"Remote" = "#1f77b4",
"Hybrid" = "#2ca02c",
"On-site" = "#ff7f0e",
"Unspecified" = "#999999"
))
# Save chart
ggsave("visualizations/remote_vs_onsite.png", plot, width = 6, height = 6)
cat("Updated remote/on-site chart saved to visualizations/remote_vs_onsite.png\n")
setwd("/Users/hui/github/us-job-market-analysis")
library(tidyverse)
library(readr)
# Load cleaned job data
job_data <- read_csv("data/cleaned_job_data.csv")
# Count distribution from 'onsite_remote' column
remote_summary <- job_data %>%
filter(!is.na(onsite_remote)) %>%
count(onsite_remote, name = "n") %>%
mutate(onsite_remote = str_to_title(onsite_remote))  # e.g. "remote" -> "Remote"
# Print summary
print(remote_summary)
# Create output folder if needed
if (!dir.exists("visualizations")) dir.create("visualizations")
# Plot pie chart
plot <- ggplot(remote_summary, aes(x = "", y = n, fill = onsite_remote)) +
geom_col(width = 1) +
coord_polar("y") +
theme_void() +
labs(title = "Job Location Type Distribution") +
scale_fill_manual(values = c(
"Remote" = "#1f77b4",
"Hybrid" = "#2ca02c",
"Onsite" = "#ff7f0e"
))
# Save chart
ggsave("visualizations/remote_vs_onsite.png", plot, width = 6, height = 6)
cat("Pie chart based on 'onsite_remote' saved to visualizations/remote_vs_onsite.png\n")
setwd("/Users/hui/github/us-job-market-analysis")
library(tidyverse)
library(readr)
library(scales)
# Load cleaned job data
job_data <- read_csv("data/cleaned_job_data.csv")
# Count and calculate percent
remote_summary <- job_data %>%
filter(!is.na(onsite_remote)) %>%
count(onsite_remote, name = "n") %>%
mutate(
onsite_remote = str_to_title(onsite_remote),
percent = n / sum(n),
label = paste0(onsite_remote, ": ", percent(percent))
)
# Print summary
print(remote_summary)
# Create output folder if needed
if (!dir.exists("visualizations")) dir.create("visualizations")
# Plot pie chart with percentage labels
plot <- ggplot(remote_summary, aes(x = "", y = n, fill = onsite_remote)) +
geom_col(width = 1, color = "white") +
coord_polar("y") +
theme_void() +
labs(title = "Job Location Type Distribution") +
geom_text(aes(label = label), position = position_stack(vjust = 0.5), size = 4.5) +
scale_fill_manual(values = c(
"Remote" = "#1f77b4",
"Hybrid" = "#2ca02c",
"Onsite" = "#ff7f0e"
))
# Save chart
ggsave("visualizations/remote_vs_onsite.png", plot, width = 6, height = 6)
cat("Pie chart with percentages saved to visualizations/remote_vs_onsite.png\n")
library(tidyverse)
library(readr)
input_file <- "data/linkedin-jobs-usa.csv"
output_file <- "data/cleaned_job_data.csv"
df_raw <- read_csv(input_file)
df_clean <- df_raw %>%
rename_with(~tolower(gsub(" ", "_", .))) %>%
distinct() %>%
filter(
!is.na(title),
!is.na(company),
!is.na(location),
!is.na(description)
) %>%
mutate(across(c(title, company, location), str_trim))
write_csv(df_clean, output_file)
cat("Cleaned data saved to", output_file, "\n")
library(tidyverse)
library(tidytext)
library(wordcloud2)
library(stopwords)
library(readr)
library(htmlwidgets)
# Load cleaned job data
job_data <- read_csv("data/cleaned_job_data.csv")
# Tokenize job descriptions into words and calculate word frequency
word_df <- job_data %>%
select(title, company, location, description) %>%
unnest_tokens(word, description) %>%
filter(!word %in% stopwords("en")) %>%
filter(str_length(word) > 2) %>%
count(word, sort = TRUE)
# Print top 20 words
print(head(word_df, 20))
# Generate a word cloud from top 100 frequent words
wc <- wordcloud2(word_df[1:100, ])
# Create output folder if it doesn't exist
if (!dir.exists("visualizations")) dir.create("visualizations")
# Save the word cloud as an HTML file
saveWidget(wc, "visualizations/wordcloud.html", selfcontained = TRUE)
cat("Wordcloud saved to visualizations/wordcloud.html\n")
library(tidyverse)
library(readr)
# Load cleaned job data
job_data <- read_csv("data/cleaned_job_data.csv")
# Count number of jobs by city
city_counts <- job_data %>%
count(location, sort = TRUE) %>%
slice_max(n, n = 10)
# Print top cities
print(city_counts)
# Create output folder if it doesn't exist
if (!dir.exists("visualizations")) dir.create("visualizations")
# Plot top 10 cities
plot <- ggplot(city_counts, aes(x = reorder(location, n), y = n)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Top 10 Cities by Data Analyst Job Postings",
x = "City",
y = "Number of Jobs") +
theme_minimal()
# Save the plot
ggsave("visualizations/top_cities.png", plot, width = 8, height = 5)
cat("Top cities plot saved to visualizations/top_cities.png\n")
library(tidyverse)
library(readr)
library(scales)
# Load cleaned job data
job_data <- read_csv("data/cleaned_job_data.csv")
# Count and calculate percent
remote_summary <- job_data %>%
filter(!is.na(onsite_remote)) %>%
count(onsite_remote, name = "n") %>%
mutate(
onsite_remote = str_to_title(onsite_remote),
percent = n / sum(n),
label = paste0(onsite_remote, ": ", percent(percent))
)
# Print summary
print(remote_summary)
# Create output folder if needed
if (!dir.exists("visualizations")) dir.create("visualizations")
# Plot pie chart with percentage labels
plot <- ggplot(remote_summary, aes(x = "", y = n, fill = onsite_remote)) +
geom_col(width = 1, color = "white") +
coord_polar("y") +
theme_void() +
labs(title = "Job Location Type Distribution") +
geom_text(aes(label = label), position = position_stack(vjust = 0.5), size = 4.5) +
scale_fill_manual(values = c(
"Remote" = "#1f77b4",
"Hybrid" = "#2ca02c",
"Onsite" = "#ff7f0e"
))
# Save chart
ggsave("visualizations/remote_vs_onsite.png", plot, width = 6, height = 6)
cat("Pie chart with percentages saved to visualizations/remote_vs_onsite.png\n")
library(tidyverse)
library(readr)
# Load cleaned job data
job_data <- read_csv("data/cleaned_job_data.csv")
# Count number of jobs by company
company_counts <- job_data %>%
count(company, sort = TRUE) %>%
slice_max(n, n = 10)
# Print top 10 companies
print(company_counts)
# Create output folder if it doesn't exist
if (!dir.exists("visualizations")) dir.create("visualizations")
# Plot top companies
plot <- ggplot(company_counts, aes(x = reorder(company, n), y = n)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Top 10 Companies by Job Postings",
x = "Company",
y = "Number of Jobs") +
theme_minimal()
# Save the plot
ggsave("visualizations/top_companies.png", plot, width = 8, height = 5)
cat("Top companies plot saved to visualizations/top_companies.png\n")
library(tidyverse)
library(readr)
library(lubridate)
# Load cleaned data
job_data <- read_csv("data/cleaned_job_data.csv")
# Ensure the date column exists and is in date format
if (!"posted_date" %in% colnames(job_data)) {
stop("No 'posted_date' column found. Check your dataset.")
}
# Convert to Date format if needed
job_data <- job_data %>%
mutate(posted_date = as_date(posted_date))
# Count number of jobs per week
trend_df <- job_data %>%
mutate(posting_week = floor_date(posted_date, "week")) %>%
count(posting_week)
# Create output folder if it doesn't exist
if (!dir.exists("visualizations")) dir.create("visualizations")
# Plot time series
plot <- ggplot(trend_df, aes(x = posting_week, y = n)) +
geom_line(color = "steelblue", size = 1) +
geom_point(size = 2, color = "darkblue") +
labs(title = "Weekly Trend of Data Analyst Job Postings",
x = "Week Posted",
y = "Number of Jobs") +
theme_minimal()
# Save the plot
ggsave("visualizations/posting_trend.png", plot, width = 8, height = 5)
cat("Posting trend plot saved to visualizations/posting_trend.png\n")
library(tidyverse)
library(tidytext)
library(stopwords)
library(readr)
library(ggplot2)
# Load data
job_data <- read_csv("data/cleaned_job_data.csv")
# Tokenize by company
words_by_company <- job_data %>%
select(company, description) %>%
unnest_tokens(word, description) %>%
filter(!word %in% stopwords("en")) %>%
filter(str_length(word) > 2) %>%
count(company, word, sort = TRUE)
# Compute tf-idf
tfidf <- words_by_company %>%
bind_tf_idf(word, company, n) %>%
arrange(desc(tf_idf))
# Select top 10 tf-idf words per company (limit to top companies only to reduce clutter)
top_companies <- job_data %>%
count(company, sort = TRUE) %>%
slice_max(n, n = 5) %>%
pull(company)
top_tfidf <- tfidf %>%
filter(company %in% top_companies) %>%
group_by(company) %>%
slice_max(tf_idf, n = 5) %>%
ungroup()
# Create output folder if needed
if (!dir.exists("visualizations")) dir.create("visualizations")
# Plot
plot <- ggplot(top_tfidf, aes(x = tf_idf, y = reorder(word, tf_idf), fill = company)) +
geom_col(show.legend = FALSE) +
facet_wrap(~company, scales = "free_y") +
labs(title = "Top TF-IDF Keywords by Company",
x = "TF-IDF Score",
y = "Keyword") +
theme_minimal()
# Save plot
ggsave("visualizations/tfidf_keywords.png", plot, width = 10, height = 6)
cat("TF-IDF keyword chart saved to visualizations/tfidf_keywords.png\n")
