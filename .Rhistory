lbls_count <- paste(lbls, ct, sep = " ")
# Pie Chart 2 – Show percentages
pct <- round(ct / sum(ct) * 100)
lbls_percent <- paste(lbls, pct, "%", sep = " ")
# Chart 1 – Pie by count
pie(ct,
labels = lbls_count,
col = c("red", "blue", "green", "yellow", "purple"),
main = "Pie Chart by Number of Students")
# Chart 2 – Pie by percentage
# MATH 3070 – A1
# Name: Hui Wang
# UID: u1303081
# Date: 2025-05-14
ct <- c(12, 5, 48, 25, 10)
lbls <- c("Arts", "Business", "Engineering", "Math", "Physics")
# Pie Chart 1 – Show counts
lbls_count <- paste(lbls, ct, sep = " ")
# Pie Chart 2 – Show percentages
pct <- round(ct / sum(ct) * 100)
lbls_percent <- paste(lbls, pct, "%", sep = " ")
# Chart 1 – Pie by count
pie(ct,
labels = lbls_count,
col = c("red", "blue", "green", "yellow", "purple"),
main = "Pie Chart by Number of Students")
# Chart 2 – Pie by percentage
pie(ct,
labels = lbls_percent,
col = rainbow(length(lbls_percent)),
main = "Pie Chart by Percentage")
# MATH 3070 – A1
# Name: Hui Wang
# UID: u1303081
# Date: 2025-05-14
ct <- c(12, 5, 48, 25, 10)
lbls <- c("Arts", "Business", "Engineering", "Math", "Physics")
# Pie Chart 1 – Show counts
lbls_count <- paste(lbls, ct, sep = " ")
# Pie Chart 2 – Show percentages
pct <- round(ct / sum(ct) * 100)
lbls_percent <- paste(lbls, pct, "%", sep = " ")
# Chart 1 – Pie by count
pie(ct,
labels = lbls_count,
col = c("red", "blue", "green", "yellow", "purple"),
main = "Pie Chart by Number of Students")
# Chart 2 – Pie by percentage
pie(ct,
labels = lbls_percent,
col = rainbow(length(lbls_percent)),
main = "Pie Chart by Percentage")
# MATH 3070 – A1
# Name: Hui Wang
# UID: u1303081
# Date: 2025-05-14
# Set layout: show two pie charts side by side
par(mfrow = c(1, 2))
# Define student counts by major
ct <- c(12, 5, 48, 25, 10)
lbls <- c("Arts", "Business", "Engineering", "Math", "Physics")
# Create labels showing counts (e.g., "Arts 12")
lbls_count <- paste(lbls, ct, sep = " ")
# Create labels showing percentages (e.g., "Arts 10%")
pct <- round(ct / sum(ct) * 100)
lbls_percent <- paste(lbls, pct, "%", sep = " ")
# Pie Chart 1 – by number of students
pie(ct,
labels = lbls_count,
col = c("red", "blue", "green", "yellow", "purple"),
main = "Pie Chart by Number of Students")
# Pie Chart 2 – by percentage
pie(ct,
labels = lbls_percent,
col = rainbow(length(lbls_percent)),
main = "Pie Chart by Percentage")
# MATH 3070 – A1
# Name: Hui Wang
# UID: u1303081
# Date: 2025-05-14
# Define student counts by major
ct <- c(12, 5, 48, 25, 10)
lbls <- c("Arts", "Business", "Engineering", "Math", "Physics")
# Create labels showing counts (e.g., "Arts 12")
lbls_count <- paste(lbls, ct, sep = " ")
# Create labels showing percentages (e.g., "Arts 10%")
pct <- round(ct / sum(ct) * 100)
lbls_percent <- paste(lbls, pct, "%", sep = " ")
# Pie Chart 1 – by number of students
pie(ct,
labels = lbls_count,
col = c("red", "blue", "green", "yellow", "purple"),
main = "Pie Chart by Number of Students")
# Pie Chart 2 – by percentage
pie(ct,
labels = lbls_percent,
col = rainbow(length(lbls_percent)),
main = "Pie Chart by Percentage")
# MATH 3070 – A1
# Name: Hui Wang
# UID: u1303081
# Date: 2025-05-14
# Define student counts by major
ct <- c(12, 5, 48, 25, 10)
lbls <- c("Arts", "Business", "Engineering", "Math", "Physics")
# Create labels showing counts (e.g., "Arts 12")
lbls_count <- paste(lbls, ct, sep = " ")
# Create labels showing percentages (e.g., "Arts 10%")
pct <- round(ct / sum(ct) * 100)
lbls_percent <- paste(lbls, pct, "%", sep = " ")
# Pie Chart 1 – by number of students
pie(ct,
labels = lbls_count,
col = c("red", "blue", "green", "yellow", "purple"),
main = "Pie Chart by Number of Students")
# Pie Chart 2 – by percentage
pie(ct,
labels = lbls_percent,
col = rainbow(length(lbls_percent)),
main = "Pie Chart by Percentage")
# Pie Chart 2 – by percentage
pie(ct,
labels = lbls_percent,
col = rainbow(length(lbls_percent)),
main = "Pie Chart by Percentage")
# Pie Chart 1 – by number of students
pie(ct,
labels = lbls_count,
col = c("red", "blue", "green", "yellow", "purple"),
main = "Pie Chart by Number of Students")
# Pie Chart 2 – by percentage
pie(ct,
labels = lbls_percent,
col = rainbow(length(lbls_percent)),
main = "Pie Chart by Percentage")
# MATH 3070 – A1
# Name: Hui Wang
# UID: u1303081
# Date: 2025-05-14
# Define student counts by major
ct <- c(12, 5, 48, 25, 10)
lbls <- c("Arts", "Business", "Engineering", "Math", "Physics")
# Create labels showing counts
lbls_count <- paste(lbls, ct, sep = " ")
# Create labels showing percentages
pct <- round(ct / sum(ct) * 100)
lbls_percent <- paste(lbls, pct, "%", sep = " ")
# Pie Chart 1 – by number of students
pie(ct,
labels = lbls_count,
col = c("red", "blue", "green", "yellow", "purple"),
main = "Pie Chart by Number of Students")
# Pie Chart 2 – by percentage
pie(ct,
labels = lbls_percent,
col = rainbow(length(lbls_percent)),
main = "Pie Chart by Percentage")
# Visualize clusters
fviz_cluster(kmeans_result, data = scaled_df,
geom = "point",
ellipse.type = "norm",
ggtheme = theme_minimal(),
main = "K-Means Clustering of E-Moped Users")
install.packages("ggplot2")
install.packages("dplyr")
install.packages("cluster")
install.packages("factoextra")
# Load libraries
library(ggplot2)
library(dplyr)
library(cluster)
library(factoextra)
# Simulated user behavior data
set.seed(42)
df <- data.frame(
avg_duration = c(rnorm(50, mean = 12, sd = 3), rnorm(50, mean = 30, sd = 4), rnorm(50, mean = 20, sd = 2)),
rides_per_week = c(rnorm(50, mean = 10, sd = 2), rnorm(50, mean = 3, sd = 1), rnorm(50, mean = 6, sd = 2)),
weekend_ratio = c(rnorm(50, mean = 0.2, sd = 0.05), rnorm(50, mean = 0.8, sd = 0.1), rnorm(50, mean = 0.5, sd = 0.1))
)
# Scale the data
scaled_df <- scale(df)
# Run k-means clustering
set.seed(123)
kmeans_result <- kmeans(scaled_df, centers = 3, nstart = 25)
# Add cluster labels
df$cluster <- as.factor(kmeans_result$cluster)
# Visualize clusters
fviz_cluster(kmeans_result, data = scaled_df,
geom = "point",
ellipse.type = "norm",
ggtheme = theme_minimal(),
main = "K-Means Clustering of E-Moped Users")
# Cluster summary
cluster_summary <- df %>%
group_by(cluster) %>%
summarise(
avg_duration = mean(avg_duration),
rides_per_week = mean(rides_per_week),
weekend_ratio = mean(weekend_ratio)
)
print(cluster_summary)
# Plot heatmap
ggplot(ride_matrix, aes(x = hour, y = weekday, fill = ride_count)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "#f0f9e8", high = "#0868ac") +
labs(
title = "Ride Frequency Heatmap by Hour and Weekday",
x = "Hour of Day",
y = "Day of Week",
fill = "Ride Count"
) +
theme_minimal()
Install packages if needed:
Install packages if needed:
#
# ============================================
# Ride Time Heatmap – Shared E-Moped Project
# Author: Hui Wang
# Visualizes ride frequency by weekday and hour
# ============================================
# Install packages if needed:
install.packages("ggplot2")
install.packages("dplyr")
install.packages("lubridate")
library(ggplot2)
library(dplyr)
library(lubridate)
# Simulate sample ride start times (300 rows)
set.seed(123)
df <- data.frame(
ride_id = 1:300,
start_time = as.POSIXct("2024-06-01 00:00:00") + runif(300, min = 0, max = 7*24*60*60)
)
df <- df %>%
mutate(
weekday = wday(start_time, label = TRUE, abbr = FALSE),  # Monday to Sunday
hour = hour(start_time)
)
# Aggregate counts
ride_matrix <- df %>%
group_by(weekday, hour) %>%
summarise(ride_count = n(), .groups = "drop")
# Plot heatmap
ggplot(ride_matrix, aes(x = hour, y = weekday, fill = ride_count)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "#f0f9e8", high = "#0868ac") +
labs(
title = "Ride Frequency Heatmap by Hour and Weekday",
x = "Hour of Day",
y = "Day of Week",
fill = "Ride Count"
) +
theme_minimal()
install.packages("dplyr")
install.packages("ggplot2")
# ============================================
# Ride Time Heatmap – Shared E-Moped Project
# Author: Hui Wang
# Visualizes ride frequency by weekday and hour
# ============================================
# Install packages if needed:
# install.packages("ggplot2")
# install.packages("dplyr")
# install.packages("lubridate")
library(ggplot2)
library(dplyr)
library(lubridate)
# Simulate sample ride start times (300 rows)
set.seed(123)
df <- data.frame(
ride_id = 1:300,
start_time = as.POSIXct("2024-06-01 00:00:00") + runif(300, min = 0, max = 7*24*60*60)
)
df <- df %>%
mutate(
weekday = wday(start_time, label = TRUE, abbr = FALSE),  # Monday to Sunday
hour = hour(start_time)
)
# Aggregate counts
ride_matrix <- df %>%
group_by(weekday, hour) %>%
summarise(ride_count = n(), .groups = "drop")
# Plot heatmap
ggplot(ride_matrix, aes(x = hour, y = weekday, fill = ride_count)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "#f0f9e8", high = "#0868ac") +
labs(
title = "Ride Frequency Heatmap by Hour and Weekday",
x = "Hour of Day",
y = "Day of Week",
fill = "Ride Count"
) +
theme_minimal()
# ============================================
# R Script: Skill Word Frequency & Wordcloud
# Author: Hui Wang
# Description: Extract frequent keywords from job descriptions
# ============================================
library(tidyverse)
# ============================================
# R Script: Skill Word Frequency & Wordcloud
# Author: Hui Wang
# Description: Extract frequent keywords from job descriptions
# ============================================
library(tidyverse)
# ============================================
# R Script: Skill Word Frequency & Wordcloud
# Author: Hui Wang
# Description: Extract frequent keywords from job descriptions
# ============================================
library(tidyverse)
# Load cleaned job data (adjust path as needed)
job_data <- read_csv("/Users/hui/2025summer/capstone2/cleaned_job_data.csv")
# ============================================
# R Script: Skill Word Frequency & Wordcloud
# Author: Hui Wang
# Description: Extract frequent keywords from job descriptions
# ============================================
library(tidyverse)
# ============================================
# R Script: Skill Word Frequency & Wordcloud
# Author: Hui Wang
# Description: Extract frequent keywords from job descriptions
# ============================================
library(tidyverse)
# ============================================
# R Script: Skill Word Frequency & Wordcloud
# Author: Hui Wang
# Description: Extract frequent keywords from job descriptions
# ============================================
install.packages("tidyverse")
library(tidyverse)
library(tidytext)
library(tidyverse)
library(tidytext)
# ============================================
# R Script: Skill Word Frequency & Wordcloud
# Author: Hui Wang
# Description: Extract frequent keywords from job descriptions
# ============================================
install.packages(c("tidyverse", "tidytext", "wordcloud2", "stopwords", "lubridate"))
library(tidyverse)
library(tidytext)
install.packages(c("tidyverse", "tidytext", "wordcloud2", "stopwords", "lubridate"))
library(tidyverse)
library(tidytext)
library(wordcloud2)
library(stopwords)
# Load cleaned job data (adjust path as needed)
job_data <- read_csv("/Users/hui/2025summer/capstone2/cleaned_job_data.csv")
library(tidyverse)
library(tidytext)
library(wordcloud2)
library(stopwords)
# Load cleaned job data (adjust path as needed)
job_data <- read_csv("/Users/hui/2025summer/capstone2/cleaned_job_data.csv")
# Unnest job descriptions into individual words
word_df <- job_data %>%
select(job_title, company_name, location, description) %>%
unnest_tokens(word, description) %>%
filter(!word %in% stopwords("en")) %>%      # Remove common English stopwords
filter(str_length(word) > 2) %>%            # Remove very short words
count(word, sort = TRUE)
# ============================================
# R Script: Skill Word Frequency & Wordcloud
# Author: Hui Wang
# Description: Extract frequent keywords from job descriptions
# ============================================
# Optional: Install packages (only run once)
# install.packages(c("tidyverse", "tidytext", "wordcloud2", "stopwords", "lubridate"))
# Load required libraries
library(tidyverse)
library(tidytext)
library(wordcloud2)
library(stopwords)
library(readr)
# Load cleaned job data (update path if needed)
job_data <- read_csv("/Users/hui/2025summer/capstone2/cleaned_job_data.csv")
# Unnest job descriptions into individual words
# NOTE: Use actual column names in your dataset
word_df <- job_data %>%
select(title, company, location, description) %>%
unnest_tokens(word, description) %>%                     # Tokenize description into words
filter(!word %in% stopwords("en")) %>%                   # Remove common English stopwords
filter(str_length(word) > 2) %>%                         # Remove very short words (like "in", "to")
count(word, sort = TRUE)                                 # Count word frequencies
# View top 20 keywords
print(head(word_df, 20))
# Generate word cloud for top 100 words
wordcloud2(word_df[1:100, ])
getwd()
# ============================================
# R Script: Skill Word Frequency & Wordcloud
# Author: Hui Wang
# Description: Extract frequent keywords from job descriptions
# ============================================
# Optional: Install packages (only run once)
# install.packages(c("tidyverse", "tidytext", "wordcloud2", "stopwords", "lubridate"))
# Load required libraries
library(tidyverse)
library(tidytext)
library(wordcloud2)
library(stopwords)
library(readr)
# Load cleaned job data (update path if needed)
job_data <- read_csv("/Users/hui/2025summer/capstone2/cleaned_job_data.csv")
# Unnest job descriptions into individual words
# NOTE: Use actual column names in your dataset
word_df <- job_data %>%
select(title, company, location, description) %>%
unnest_tokens(word, description) %>%                     # Tokenize description into words
filter(!word %in% stopwords("en")) %>%                   # Remove common English stopwords
filter(str_length(word) > 2) %>%                         # Remove very short words (like "in", "to")
count(word, sort = TRUE)                                 # Count word frequencies
# View top 20 keywords
print(head(word_df, 20))
# Generate word cloud for top 100 words
wordcloud2(word_df[1:100, ])
# ============================================
# R Script: Skill Word Frequency & Wordcloud
# Author: Hui Wang
# Description: Extract frequent keywords from job descriptions
# ============================================
# Optional: Install packages (only run once)
# install.packages(c("tidyverse", "tidytext", "wordcloud2", "stopwords", "lubridate"))
# Load required libraries
library(tidyverse)
library(tidytext)
library(wordcloud2)
library(stopwords)
library(readr)
# Load cleaned job data (update path if needed)
job_data <- read_csv("/Users/hui/2025summer/capstone2/cleaned_job_data.csv")
# Unnest job descriptions into individual words
# NOTE: Use actual column names in your dataset
word_df <- job_data %>%
select(title, company, location, description) %>%
unnest_tokens(word, description) %>%                     # Tokenize description into words
filter(!word %in% stopwords("en")) %>%                   # Remove common English stopwords
filter(str_length(word) > 2) %>%                         # Remove very short words (like "in", "to")
count(word, sort = TRUE)                                 # Count word frequencies
# View top 20 keywords
print(head(word_df, 20))
# Generate word cloud for top 100 words
wordcloud2(word_df[1:100, ])
visualizations
# ============================================
# R Script: Skill Word Frequency & Wordcloud
# Author: Hui Wang
# Description: Extract frequent keywords from job descriptions and export as HTML
# ============================================
# Optional: Install packages (only run once)
# install.packages(c("tidyverse", "tidytext", "wordcloud2", "stopwords", "htmlwidgets"))
# Load required libraries
library(tidyverse)
library(tidytext)
library(wordcloud2)
library(stopwords)
library(htmlwidgets)
# Load cleaned job data (update path if needed)
job_data <- read_csv("/Users/hui/2025summer/capstone2/cleaned_job_data.csv")
# Unnest job descriptions into individual words
word_df <- job_data %>%
select(title, company, location, description) %>%
unnest_tokens(word, description) %>%                     # Tokenize description into words
filter(!word %in% stopwords("en")) %>%                   # Remove common English stopwords
filter(str_length(word) > 2) %>%                         # Remove very short words
count(word, sort = TRUE)                                 # Count word frequencies
# View top 20 keywords
print(head(word_df, 20))
# Generate word cloud for top 100 words
wc <- wordcloud2(word_df[1:100, ])
# Define output directory and file path
output_dir <- "/Users/hui/2025summer/capstone2/visualizations"
output_file <- "wordcloud.html"
full_path <- file.path(output_dir, output_file)
# Create the output directory if it doesn't exist
if (!dir.exists(output_dir)) {
dir.create(output_dir, recursive = TRUE)
}
# Save the word cloud as an HTML file
saveWidget(wc, file = full_path, selfcontained = TRUE)
# Open the saved HTML file in the default browser
browseURL(full_path)
# Load libraries
library(tidyverse)
library(readr)
# Read CSV
df <- read_csv("data/linkedin-jobs-usa.csv")
library(tidyverse)
library(readr)
input_file <- "data/linkedin-jobs-usa.csv"
output_file <- "data/cleaned_job_data.csv"
df_raw <- read_csv(input_file)
setwd("/Users/hui/github/us-job-market-analysis")
library(tidyverse)
library(readr)
input_file <- "data/linkedin-jobs-usa.csv"
output_file <- "data/cleaned_job_data.csv"
df_raw <- read_csv(input_file)
df_clean <- df_raw %>%
rename_with(~tolower(gsub(" ", "_", .))) %>%
distinct() %>%
filter(
!is.na(title),
!is.na(company),
!is.na(location),
!is.na(description)
) %>%
mutate(across(c(title, company, location), str_trim))
write_csv(df_clean, output_file)
cat("Cleaned data saved to", output_file, "\n")
